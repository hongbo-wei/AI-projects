# ğŸ¤– LLM Training Pipeline: CPT â†’ SFT â†’ RAG

A comprehensive Large Language Model training pipeline that demonstrates the complete workflow from Continuous Pre-Training (CPT) through Supervised Fine-Tuning (SFT) to Retrieval-Augmented Generation (RAG).

## ğŸ¯ Overview

This project implements a standard LLM training workflow that covers:

1. **CPT (Continuous Pre-Training)** - Domain-specific pre-training using Masked Language Modeling
2. **SFT (Supervised Fine-Tuning)** - Instruction following training with prompt-response pairs
3. **RAG (Retrieval-Augmented Generation)** - Knowledge-enhanced generation with retrieval capabilities

## ğŸ“ Project Structure

```
llm-training/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pretrain.txt              # CPT input: unsupervised text
â”‚   â”œâ”€â”€ sft.jsonl                 # SFT input: prompt + response pairs
â”‚   â””â”€â”€ rag.jsonl                 # RAG input: question + answer pairs
â”œâ”€â”€ output/                       # Generated model outputs
â”‚   â”œâ”€â”€ cpt/                      # CPT trained model
â”‚   â”œâ”€â”€ sft/                      # SFT trained model
â”‚   â””â”€â”€ rag/                      # RAG trained model
â”œâ”€â”€ train_cpt.py                  # CPT training script
â”œâ”€â”€ train_sft.py                  # SFT training script
â”œâ”€â”€ train_rag.py                  # RAG training script
â”œâ”€â”€ main.py                       # Main pipeline executor
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (recommended)
- 8GB+ RAM

### Installation

1. **Clone and navigate to the project:**
```bash
cd llm-training
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Run the complete pipeline:**
```bash
# Use Python main script
python main.py

# Or run individual phases
python train_cpt.py
python train_sft.py
python train_rag.py
```

## ğŸ“Š Training Phases

### Phase 1: CPT (Continuous Pre-Training)
- **Model**: BERT-base-uncased
- **Objective**: Masked Language Modeling (MLM)
- **Input**: Raw text data (`pretrain.txt`)
- **Output**: Domain-adapted language model

**Key Features:**
- 15% token masking probability
- Domain-specific vocabulary adaptation
- Contextual understanding improvement

### Phase 2: SFT (Supervised Fine-Tuning)
- **Model**: GPT-2
- **Objective**: Causal Language Modeling
- **Input**: Prompt-response pairs (`sft.jsonl`)
- **Output**: Instruction-following model

**Key Features:**
- Human-AI conversation format
- Response quality optimization
- Instruction following capabilities

### Phase 3: RAG (Retrieval-Augmented Generation)
- **Model**: RAG-sequence-nq
- **Objective**: Knowledge-enhanced generation
- **Input**: Question-answer pairs (`rag.jsonl`)
- **Output**: Retrieval-augmented model

**Key Features:**
- External knowledge integration
- Factual accuracy improvement
- Dynamic information retrieval

## ğŸ“‹ Data Formats

### CPT Data (`pretrain.txt`)
```
Artificial Intelligence is transforming the world.
Machine learning is a subfield of AI.
Deep learning uses neural networks to process data.
...
```

### SFT Data (`sft.jsonl`)
```json
{"prompt": "What is AI?", "response": "Artificial Intelligence is a field..."}
{"prompt": "Who are you?", "response": "I'm an AI language model..."}
...
```

### RAG Data (`rag.jsonl`)
```json
{"question": "When was Google founded?", "answer": "Google was founded in 1998..."}
{"question": "What is the capital of France?", "answer": "The capital of France is Paris..."}
...
```

## âš™ï¸ Configuration

### Training Parameters

Each phase can be customized by modifying the `TrainingArguments` in respective scripts:

```python
training_args = TrainingArguments(
    output_dir="output/cpt",
    per_device_train_batch_size=8,
    num_train_epochs=3,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=100,
    fp16=True,  # Enable mixed precision
)
```

### Hardware Requirements

| Phase | Min GPU Memory | Recommended | Batch Size |
|-------|---------------|-------------|------------|
| CPT   | 4GB          | 8GB+        | 8          |
| SFT   | 6GB          | 12GB+       | 2          |
| RAG   | 8GB          | 16GB+       | 1          |

##  Model Outputs

After training, models are saved in the `output/` directory:

```
output/
â”œâ”€â”€ cpt/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ tokenizer.json
â”œâ”€â”€ sft/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ tokenizer.json
â””â”€â”€ rag/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â””â”€â”€ tokenizer.json
```

## ğŸ” Usage Examples

### Loading Trained Models

```python
from transformers import AutoTokenizer, AutoModel

# Load CPT model
cpt_tokenizer = AutoTokenizer.from_pretrained("output/cpt")
cpt_model = AutoModel.from_pretrained("output/cpt")

# Load SFT model
sft_tokenizer = AutoTokenizer.from_pretrained("output/sft")
sft_model = AutoModel.from_pretrained("output/sft")
```

### Inference Example

```python
# Example with SFT model
from transformers import pipeline

generator = pipeline("text-generation", 
                    model="output/sft", 
                    tokenizer="output/sft")

prompt = "Human: What is machine learning?\nAssistant:"
response = generator(prompt, max_length=100, num_return_sequences=1)
print(response[0]['generated_text'])
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce batch size
   - Enable gradient checkpointing
   - Use mixed precision training (fp16)

2. **RAG Training Failures**
   - Normal for demo setup
   - Requires proper index configuration
   - Need larger datasets for production

3. **Import Errors**
   - Ensure all dependencies are installed
   - Check Python version compatibility
   - Verify CUDA setup for GPU training

### Performance Tips

- Use GPU for training (set `fp16=True`)
- Adjust batch size based on available memory
- Use gradient accumulation for larger effective batch sizes
- Monitor training with TensorBoard logs

## ğŸ—ºï¸ LLMä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹å›¾

```mermaid
flowchart LR
    %% æ•°æ®å‡†å¤‡
    A1[pretrain.txt<br>æ— ç›‘ç£æ–‡æœ¬]:::data1
    A2[sft.jsonl<br>æŒ‡ä»¤å¯¹è¯]:::data2
    A3[rag.jsonl<br>çŸ¥è¯†é—®ç­”]:::data3

    %% é˜¶æ®µ
    B[CPT é¢„è®­ç»ƒ<br>BERT + MLM]:::stage
    C[output/cpt/<br>CPTæ¨¡å‹]:::output
    D[SFT å¾®è°ƒ<br>GPT-2 + æŒ‡ä»¤]:::stage
    E[output/sft/<br>SFTæ¨¡å‹]:::output
    F[RAG æ£€ç´¢å¢å¼º<br>RAG-sequence-nq]:::stage
    G[output/rag/<br>RAGæ¨¡å‹]:::output

    %% è¿æ¥
    A1 --> B --> C --> D --> E --> F --> G
    A2 --> D
    A3 --> F

    %% æ ·å¼
    classDef data1 fill:#ffb3ba,stroke:#fff,stroke-width:2px,color:#222,font-weight:bold;
    classDef data2 fill:#bae1ff,stroke:#fff,stroke-width:2px,color:#222,font-weight:bold;
    classDef data3 fill:#baffc9,stroke:#fff,stroke-width:2px,color:#222,font-weight:bold;
    classDef stage fill:#222,stroke:#fff,stroke-width:2px,color:#ffe066,font-size:16px,font-weight:bold;
    classDef output fill:#444,stroke:#ffe066,stroke-width:2px,color:#ffe066,font-weight:bold;
    class A1 data1;
    class A2 data2;
    class A3 data3;
    class B,D,F stage;
    class C,E,G output;
```

## ğŸ“š References

- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Hugging Face Datasets](https://huggingface.co/datasets/)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [RAG Paper](https://arxiv.org/abs/2005.11401)

## ğŸ¤ Contributing

Feel free to submit issues, feature requests, or pull requests to improve this training pipeline.

## ğŸ“„ License

This project is open source and available under the MIT License.

---

**Note**: This is a demonstration pipeline using the exact code structure from the original specification. The training scripts use minimal configurations suitable for demo purposes.
